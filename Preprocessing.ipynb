{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Preprocessing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMn4NZpsPxjRvhQE+akjTWa"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"zBitKlsp4rkf"},"source":["import os\n","import logging\n","\n","from PIL import Image\n","from tqdm import tqdm\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","import json\n","\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y_vUixHg448h"},"source":["# we get the data cloning our github repository\n","\n","if not os.path.isdir('./COCO'):\n","  !git clone https://github.com/MarcoSaponara/MLAI_LINKS_project.git\n","  !mv 'Conditional_Text_Generation_Project' 'CTRL'\n","\n","with open(\"CTRL/annotations_train_val_2014/captions_train2014.json\",\"r\") as f:\n","  train_dataset = json.load(f)\n","\n","with open(\"CTRL/annotations_train_val_2014/captions_val2014.json\",\"r\") as f:\n","  val_dataset = json.load(f)\n","\n","with open(\"CTRL/annotations_test_2014/image_info_test2014.json\",\"r\") as f:\n","  test_dataset = json.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ILNtsXVsqbWm"},"source":["ds = CocoDataset(path='COCO/annotations_train_val_2014/captions_train2014.json', text_field=data.Field())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xshOYoNm5m3v"},"source":["# dataset description\n","\n","print(train_dataset.keys())\n","print(\" info is a dict that gives info about the dataset \\n licenses is a list of licenses related to the source of the images \")\n","print(\"\\n\")\n","print(\"images is a list of dictionaries , each dict is a photo and contains basic info like the url, dimensions and id\")\n","print(train_dataset['images'][0].keys(),len(train_dataset['images']), \" elements\")\n","print(\"\\n\")\n","print(\"annotations is a list of dictionaries, each dict is a caption\")\n","print(train_dataset['annotations'][0].keys(),len(train_dataset['annotations']), \" elements\")\n","print(\"there are much more captions than images so each image has more than one caption\")\n","print(\"\\n\")\n","print(\"test set is the same but instead of the annotations it provides us the categories of the images it contains\")\n","print(test_dataset.keys(), len(test_dataset['images']))\n","print(\"categories is a list of dictionaries, each dict is one of the categories\")\n","print(test_dataset['categories'][0].keys())\n","print(\"supercategory is a general category while name is a more specific one\")\n","print(\"example: \",test_dataset['categories'][1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ccUvc7o85pmG"},"source":["N_images = len(train_dataset['images'])\n","lista_im = []\n","for i in range(N_images):\n","  lista_im.append(train_dataset['images'][i]['id'])\n","lista_im.sort()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GMu0cQtF6Twe"},"source":["N_capts = len(train_dataset['annotations'])\n","lista_cap = []\n","lista_im_cap = []\n","for i in range(N_capts):\n","  lista_cap.append(train_dataset['annotations'][i]['caption'])\n","  lista_im_cap.append(train_dataset['annotations'][i]['image_id'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MvsOMBe16WHU"},"source":["immagini = pd.DataFrame(data=lista_im, columns=['image_id']) # DF with images' IDs\n","captions = pd.DataFrame(data=lista_im_cap, columns=['image_id']) # DF with... \n","captions['capt'] = lista_cap #... captions and their corresponding image\n","capt_image = pd.merge(captions,immagini) # DF with all the captions of all the images\n","capt_image_group = capt_image.groupby('image_id') # grouped DF by image id\n","capt_image.head(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"621vWssvluQb"},"source":["names = set()\n","for item in test_dataset['categories']:\n","  names.add(item['name'])\n","print(names)\n","supercategories = set()\n","for item in test_dataset['categories']:\n","  supercategories.add(item['supercategory'])\n","print(supercategories)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tf_kFFf-pqpd"},"source":["full_objects_list = pd.read_csv(\"COCO/full_objects_list.txt\", sep=';')\n","full_objects_list = full_objects_list.rename(columns={'ID':'id', 'Object (Paper)':'name', 'Super Category':'supercategory'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RCDadjjOMMBq"},"source":["def check_caption(cap):\n","  ids = list() # to store the ids of the categories related to the caption\n","\n","  #for cat in test_dataset['categories']:\n","  for i, cat in full_objects_list.iterrows():\n","    if cat['supercategory'] == 'person':\n","      synonyms = list()\n","      synonyms.append('person')\n","      synonyms.append('man')\n","      synonyms.append('woman')\n","      synonyms.append('couple')\n","      synonyms.append('group')\n","      synonyms.append('people')\n","      synonyms.append('girl')\n","      synonyms.append('boy')\n","      \n","      for syn in synonyms:\n","        if syn in cap:\n","          ids.append(cat['id'])\n","          break\n","\n","    else:\n","      if cat['supercategory'] in cap:\n","        ids.append(cat['id'])\n","      \n","  if ids is not None: # look for names in categories\n","    #for cat in test_dataset['categories']:\n","    for i, cat in full_objects_list.iterrows():\n","      if cat['name'] in cap:\n","        ids.append(cat['id'])\n","\n","  return ids\n","\n","def lemmarize(text):\n","  wnl = WordNetLemmatizer()\n","  tokens = [token.lower() for token in word_tokenize(text)]\n","  lemmatized_words = [wnl.lemmatize(token) for token in tokens]\n","  return lemmatized_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTcwlu9ypNO-"},"source":["for annot in train_dataset['annotations']:\n","  annot['lab_ids'] = check_caption(lemmarize(annot['caption']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2MKwO2jEIBn","executionInfo":{"elapsed":579,"status":"ok","timestamp":1612404259296,"user":{"displayName":"Giulio Cerruto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2Eu1p6y3oJGuXwKvRHcooa_OFElVoGD8oA7WiA=s64","userId":"15268458970073686622"},"user_tz":-60},"outputId":"2e6f4cb3-638d-45bc-b7da-f86f84c12a0c"},"source":["unlabeled = 0\n","for an in train_dataset['annotations']:\n","  if not an['lab_ids']:\n","    unlabeled = unlabeled + 1\n","\n","print('unlabeled captions: ' + str(100*unlabeled/len(train_dataset['annotations']))+'%')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["unlabeled captions: 18.2071077218054%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BtMKL41E2cor"},"source":["catlabs = {new_list: [] for new_list in full_objects_list['id']}\n","\n","for annot in train_dataset['annotations']:\n","  for id in annot['lab_ids']:\n","    catlabs[id].append(annot['id'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2jnVTTK5us0"},"source":["with open('./train_labels.json', 'w') as fp:\n","    json.dump(catlabs, fp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qkqfu6FLtOq6"},"source":["### DATASET GENERATION"]},{"cell_type":"code","metadata":{"id":"Ir1Rc5Th8Mbe"},"source":["with open('CTRL/annotations_train_val_2014/train_labels.json','r') as f:\n","  ds = json.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPjz_aa_pjQj","executionInfo":{"status":"ok","timestamp":1622477962950,"user_tz":-120,"elapsed":352,"user":{"displayName":"Giulio Cerruto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2Eu1p6y3oJGuXwKvRHcooa_OFElVoGD8oA7WiA=s64","userId":"15268458970073686622"}},"outputId":"9afc62d2-81de-4acf-c77a-3b1eafca3c62"},"source":["len(ds['1']) # size of dataset with label 'person'"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["164688"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"BGXQoZfDA9q_"},"source":["# creating datasets for training\n","for k in range(4):\n","  output_dict = [x['caption'] for x in train_dataset['annotations'] if x['id'] in ds['1'][k*10000:(k+1)*10000]]\n","  with open('CTRL/dataset10k_ +' + str(k+1) + '.txt','w') as f:\n","    for line in output_dict:\n","      f.write(line)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TvRCa08BJLyM"},"source":["# creating dataset with prompts\n","output_dict = [x['caption'] for x in train_dataset['annotations'] if x['id'] in ds['1'][-1000:]]\n","with open('CTRL/prompts.txt','w') as f:\n","  for line in output_dict:\n","    line = line.split(' ')\n","    f.write('Person '+ line[0] + ' ' + line[1]+'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cJ7m49tWOlMj"},"source":["# creating sample dataset for evaluation\n","output_dict = [x['caption'] for x in train_dataset['annotations'] if x['id'] in ds['1'][40000:41000]]\n","with open('CTRL/sample_dataset.txt','w') as f:\n","  for line in output_dict:\n","    f.write(line+'\\n')"],"execution_count":null,"outputs":[]}]}